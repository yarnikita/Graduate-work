{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import warnings\n",
    "import numpy as np\n",
    "final = pd.read_csv('final_data.csv', delimiter=';')\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for building machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_split(data, t_size, v_size):\n",
    "    return np.split(\n",
    "        data.sample(frac=1, random_state=123),\n",
    "        [int(len(data)*t_size/100),\n",
    "         int(len(data)*(t_size+v_size)/100)]\n",
    "    )\n",
    "\n",
    "train_size = 70\n",
    "val_size = 20\n",
    "\n",
    "train, valid, test = triple_split(final, train_size, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train.drop(['target1', 'valuation_currency_code', 'valuation_amount', 'raised_amount', 'raised_currency_code', 'investment_rounds'], axis=1)\n",
    "y_train=train['target1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val=valid.drop(['target1', 'valuation_currency_code', 'valuation_amount', 'raised_amount', 'raised_currency_code', 'investment_rounds'], axis=1)\n",
    "y_val=valid['target1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=test.drop(['target1', 'valuation_currency_code', 'valuation_amount', 'raised_amount', 'raised_currency_code', 'investment_rounds'], axis=1)\n",
    "y_test=test['target1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=[cols])\n",
    "\n",
    "X_test = pd.DataFrame(X_test, columns=[cols])\n",
    "\n",
    "X_val = pd.DataFrame(X_val, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv ('X_unbalanced.csv')\n",
    "y_train.to_csv ('y_unbalanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TomekLinks(sampling_strategy='majority')\n",
    "X_tl, y_tl = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "X_tl = scaler.fit_transform(X_tl)\n",
    "X_tl= pd.DataFrame(X_tl, columns=[cols])\n",
    "\n",
    "X_tl.to_csv ('X_balanced by TL.csv')\n",
    "y_tl.to_csv ('y_balanced by TL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTETomek(sampling_strategy='minority')\n",
    "X_stl, y_stl= smt.fit_resample (X_train, y_train)\n",
    "\n",
    "X_stl = scaler.fit_transform(X_stl)\n",
    "X_stl= pd.DataFrame(X_stl, columns=[cols])\n",
    "\n",
    "X_stl.to_csv ('X_balanced by STL.csv')\n",
    "y_stl.to_csv ('y_balanced by STL.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('X_unbalanced.csv', delimiter=';')\n",
    "y_train=pd.read_csv('y_unbalanced.csv', delimiter=';')\n",
    "\n",
    "#X_train=pd.read_csv('X_balanced_by_TL.csv', delimiter=';')\n",
    "#y_train=pd.read_csv('y_balanced_by_TL.csv', delimiter=';')\n",
    "\n",
    "#X_train=pd.read_csv('X_balanced_by_STL.csv', delimiter=';')\n",
    "#y_train=pd.read_csv('y_balanced_by_STL.csv', delimiter=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_fin = final.columns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "final_std = scaler.fit_transform(final)\n",
    "\n",
    "final_std = pd.DataFrame(final_std, columns=[cols_fin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model_1 = sm.OLS(\n",
    "    final_std[\"target1\"],\n",
    "    final_std[[\"category_code\", \"invested_companies\", \"funding_rounds\", \"funding_total_usd\", \"milestones\", \"relationships\", \"angel\", \"crowdfunding\", \"other\", \"post_ipo\", \"series_a\", \"series_b\", \"series_c\", \"venture\", \"number_of_merges\", \"fin_org_financed\", \"person_financed\", \"offices\", \"country_code\", \"new_york\", \"california\"]],\n",
    "    family=families.Binomial(),\n",
    ").fit()\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "\n",
    "\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = logreg.predict(X_val)\n",
    "\n",
    "y_pred_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "imps = permutation_importance(logreg, X_train, y_train)\n",
    "avg_importance = np.abs(imps.importances_mean)\n",
    "avg_importance = pd.DataFrame.from_dict(avg_importance)\n",
    "avg_importance.index=[\"category_code\", \"invested_companies\", \"funding_rounds\", \"funding_total_usd\", \"milestones\", \"relationships\", \"angel\", \"crowdfunding\", \"other\", \"post_ipo\", \"series_a\", \"series_b\", \"series_c\", \"venture\", \"number_of_merges\", \"fin_org_financed\", \"person_financed\", \"offices\", \"country_code\", \"new_york\", \"california\"]\n",
    "avg_importance.columns = [\"Importance\"]\n",
    "avg_importance = avg_importance.sort_values(\"Importance\", ascending=True)\n",
    "avg_importance.plot(kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_val, y_pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = logreg.predict(X_train)\n",
    "\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[1,1])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[0,0])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[1,0])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative:0', 'Actual Positive:1'], \n",
    "                                 index=['Predict Negative:0', 'Predict Positive:1'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[1,1]\n",
    "TN = cm[0,0]\n",
    "FP = cm[1,0]\n",
    "FN = cm[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP / float(TP + FP)\n",
    "\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print('Specificity : {0:0.4f}'.format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1score = f1_score (y_val, y_pred_val)\n",
    "\n",
    "print('f1-score : {0:0.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_val, pos_label = 1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.title('ROC curve for Lofistic regression for Predicting Success')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC = roc_auc_score(y_val, y_pred_val)\n",
    "\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "cv_results = cross_validate(logreg, X_val, y_val, cv=9)\n",
    "\n",
    "score_acc = stat.mean(cross_val_score(logreg, X_val, y_val, cv = 9, scoring = 'accuracy'))  \n",
    "score_rec = stat.mean(cross_val_score(logreg, X_val, y_val, cv = 9, scoring = 'recall'))                \n",
    "score_prec = stat.mean(cross_val_score(logreg, X_val, y_val, cv = 9, scoring = 'precision'))                \n",
    "score_f1 = stat.mean(cross_val_score(logreg, X_val, y_val, cv = 9, scoring = 'f1'))                \n",
    "score_RA = stat.mean(cross_val_score(logreg, X_val, y_val, cv = 9, scoring = 'roc_auc'))                \n",
    "\n",
    "specificity = make_scorer(recall_score, pos_label=0)\n",
    "score_spec = stat.mean (cross_val_score(logreg, X_val, y_val, cv=9, scoring = specificity))\n",
    "score_FPR = 1 - score_spec\n",
    "score_error = 1 - score_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = gnb.predict(X_val)\n",
    "\n",
    "y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "imps = permutation_importance(gnb, X_train, y_train)\n",
    "avg_importance = np.abs(imps.importances_mean)\n",
    "avg_importance = pd.DataFrame.from_dict(avg_importance)\n",
    "avg_importance.index=[\"category_code\", \"invested_companies\", \"funding_rounds\", \"funding_total_usd\", \"milestones\", \"relationships\", \"angel\", \"crowdfunding\", \"other\", \"post_ipo\", \"series_a\", \"series_b\", \"series_c\", \"venture\", \"number_of_merges\", \"fin_org_financed\", \"person_financed\", \"offices\", \"country_code\", \"new_york\", \"california\"]\n",
    "avg_importance.columns = [\"Importance\"]\n",
    "avg_importance = avg_importance.sort_values(\"Importance\", ascending=True)\n",
    "avg_importance.plot(kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_val, y_pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = gnb.predict(X_train)\n",
    "\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[1,1])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[0,0])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[1,0])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative:0', 'Actual Positive:1'], \n",
    "                                 index=['Predict Negative:0', 'Predict Positive:1'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[1,1]\n",
    "TN = cm[0,0]\n",
    "FP = cm[1,0]\n",
    "FN = cm[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP / float(TP + FP)\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print('Specificity : {0:0.4f}'.format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score = f1_score (y_val, y_pred_val)\n",
    "\n",
    "print('f1-score : {0:0.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_val, pos_label = 1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.title('ROC curve for Gaussian Naive Bayes Classifier for Predicting Success')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC = roc_auc_score(y_val, y_pred_val)\n",
    "\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation on Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(gnb, X_val, y_val, cv=9)\n",
    "\n",
    "score_acc = stat.mean(cross_val_score(gnb, X_val, y_val, cv = 9, scoring = 'accuracy'))  \n",
    "score_rec = stat.mean(cross_val_score(gnb, X_val, y_val, cv = 9, scoring = 'recall'))                \n",
    "score_prec = stat.mean(cross_val_score(gnb, X_val, y_val, cv = 9, scoring = 'precision'))                \n",
    "score_f1 = stat.mean(cross_val_score(gnb, X_val, y_val, cv = 9, scoring = 'f1'))                \n",
    "score_RA = stat.mean(cross_val_score(gnb, X_val, y_val, cv = 9, scoring = 'roc_auc'))                \n",
    "\n",
    "specificity = make_scorer(recall_score, pos_label=0)\n",
    "score_spec = stat.mean (cross_val_score(gnb, X_val, y_val, cv=9, scoring = specificity))\n",
    "score_FPR = 1 - score_spec\n",
    "score_error = 1 - score_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=16, random_state=42)\n",
    "\n",
    "clf_en.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "imps = permutation_importance(clf_en, X_train, y_train)\n",
    "avg_importance = np.abs(imps.importances_mean)\n",
    "avg_importance = pd.DataFrame.from_dict(avg_importance)\n",
    "avg_importance.index=[\"category_code\", \"invested_companies\", \"funding_rounds\", \"funding_total_usd\", \"milestones\", \"relationships\", \"angel\", \"crowdfunding\", \"other\", \"post_ipo\", \"series_a\", \"series_b\", \"series_c\", \"venture\", \"number_of_merges\", \"fin_org_financed\", \"person_financed\", \"offices\", \"country_code\", \"new_york\", \"california\"]\n",
    "avg_importance.columns = [\"Importance\"]\n",
    "avg_importance = avg_importance.sort_values(\"Importance\", ascending=True)\n",
    "avg_importance.plot(kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = clf_en.predict(X_val) \\\n",
    "\n",
    "print('Model accuracy score with criterion entropy: {0:0.4f}'. format(accuracy_score(y_val, y_pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = clf_en.predict(X_train)\n",
    "\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[1,1])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[0,0])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[1,0])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative:0', 'Actual Positive:1'], \n",
    "                                 index=['Predict Negative:0', 'Predict Positive:1'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[1,1]\n",
    "TN = cm[0,0]\n",
    "FP = cm[1,0]\n",
    "FN = cm[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP / float(TP + FP)\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print('Specificity : {0:0.4f}'.format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score = f1_score (y_val, y_pred_val)\n",
    "\n",
    "print('f1-score : {0:0.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_val, pos_label = 1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.title('ROC curve for Decision Tree for Predicting Success')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC = roc_auc_score(y_val, y_pred_val)\n",
    "\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation on Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(clf_en, X_val, y_val, cv=9)\n",
    "\n",
    "score_acc = stat.mean(cross_val_score(clf_en, X_val, y_val, cv = 9, scoring = 'accuracy'))  \n",
    "score_rec = stat.mean(cross_val_score(clf_en, X_val, y_val, cv = 9, scoring = 'recall'))                \n",
    "score_prec = stat.mean(cross_val_score(clf_en, X_val, y_val, cv = 9, scoring = 'precision'))                \n",
    "score_f1 = stat.mean(cross_val_score(clf_en, X_val, y_val, cv = 9, scoring = 'f1'))                \n",
    "score_RA = stat.mean(cross_val_score(clf_en, X_val, y_val, cv = 9, scoring = 'roc_auc'))                \n",
    "\n",
    "specificity = make_scorer(recall_score, pos_label=0)\n",
    "score_spec = stat.mean (cross_val_score(clf_en, X_val, y_val, cv=9, scoring = specificity))\n",
    "score_FPR = 1 - score_spec\n",
    "score_error = 1 - score_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=16, loss_function='Logloss', random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "imps = permutation_importance(model, X_train, y_train)\n",
    "avg_importance = np.abs(imps.importances_mean)\n",
    "avg_importance = pd.DataFrame.from_dict(avg_importance)\n",
    "avg_importance.index=[\"category_code\", \"invested_companies\", \"funding_rounds\", \"funding_total_usd\", \"milestones\", \"relationships\", \"angel\", \"crowdfunding\", \"other\", \"post_ipo\", \"series_a\", \"series_b\", \"series_c\", \"venture\", \"number_of_merges\", \"fin_org_financed\", \"person_financed\", \"offices\", \"country_code\", \"new_york\", \"california\"]\n",
    "avg_importance.columns = [\"Importance\"]\n",
    "avg_importance = avg_importance.sort_values(\"Importance\", ascending=True)\n",
    "avg_importance.plot(kind='barh', figsize=(10, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Val Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[1,1])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[0,0])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[1,0])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative:0', 'Actual Positive:1'], \n",
    "                                 index=['Predict Negative:0', 'Predict Positive:1'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[1,1]\n",
    "TN = cm[0,0]\n",
    "FP = cm[1,0]\n",
    "FN = cm[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP / float(TP + FP)\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print('Specificity : {0:0.4f}'.format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score = f1_score (y_val, y_pred_val)\n",
    "\n",
    "print('f1-score : {0:0.4f}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_val, pos_label = 1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.title('ROC curve for CatBoost for Predicting Success')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC = roc_auc_score(y_val, y_pred_val)\n",
    "\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation on CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from catboost import Pool, cv\n",
    "\n",
    "# Create a CatBoost Pool\n",
    "catboost_pool = Pool(X_train, label=y_train)\n",
    "\n",
    "# Define the parameters for the CatBoost model\n",
    "params = {\n",
    "\t'iterations': 1000,\n",
    "\t'learning_rate': 0.01,\n",
    "    'depth': 16,\n",
    "\t'loss_function': 'Logloss',\n",
    "\t'random_state': 42,\n",
    "}\n",
    "\n",
    "# Perform cross-validation using the cv function from CatBoost\n",
    "cv_results, cv_model = cv(\n",
    "\tpool=catboost_pool,\n",
    "\tparams=params,\n",
    "\t# Specify the number of folds for cross-validation\n",
    "\tfold_count=9, \n",
    "\t# Print information during training\n",
    "\tverbose=False, \n",
    "\treturn_models=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def Accuracy_Score(cv_model,y_test):\n",
    "  score ={}\n",
    "  for i, model in enumerate(cv_model):\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_val.values, prediction_type='Class')\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    score[i+1]=accuracy\n",
    "    \n",
    "  return score\n",
    "\n",
    "data=Accuracy_Score(cv_model,y_test)\n",
    "stat.mean(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Error_Score(cv_model,y_test):\n",
    "  error ={}\n",
    "  for i, model in enumerate(cv_model):\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_val.values, prediction_type='Class')\n",
    "    # Calculate accuracy\n",
    "    err = 1-accuracy_score(y_val, y_pred)\n",
    "    error[i+1]=err\n",
    "    \n",
    "  return error\n",
    "    \n",
    "data=Error_Score(cv_model,y_test)\n",
    "stat.mean(data.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def Recall_Score(cv_model,y_test):\n",
    "  recall = {}\n",
    "  \n",
    "  for i, model in enumerate(cv_model):\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_val.values, prediction_type='Class')\n",
    "    # Calculate accuracy\n",
    "    rec = recall_score(y_val, y_pred)\n",
    "    recall[i+1]=rec\n",
    "    \n",
    "  return recall\n",
    "    \n",
    "data=Recall_Score(cv_model,y_val)\n",
    "stat.mean(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def Precision_Score(cv_model,y_test):\n",
    "  precision = {}\n",
    "    \n",
    "  for i, model in enumerate(cv_model):\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_val.values, prediction_type='Class')\n",
    "    # Calculate accuracy\n",
    "    prec = precision_score(y_val, y_pred)\n",
    "    precision[i+1]=prec\n",
    "    \n",
    "  return precision\n",
    "    \n",
    "data=Precision_Score(cv_model,y_val)\n",
    "stat.mean(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_Score(cv_model,y_test):\n",
    "  f1 = {}\n",
    "  \n",
    "  for i, model in enumerate(cv_model):\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_val.values, prediction_type='Class')\n",
    "    # Calculate accuracy\n",
    "    f_1 = f1_score(y_val, y_pred)\n",
    "    f1[i+1]=f_1\n",
    "    \n",
    "  return f1\n",
    "    \n",
    "data=f1_Score(cv_model,y_val)\n",
    "stat.mean(data.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "specificity = make_scorer(recall_score, pos_label=0)\n",
    "\n",
    "def Specificity_Score(cv_model,y_test):\n",
    "  spec_score = {}\n",
    "  \n",
    "  for i, model in enumerate(cv_model):\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_val.values, prediction_type='Class')\n",
    "    # Calculate accuracy\n",
    "    spec = recall_score (y_val, y_pred, pos_label=0)\n",
    "    spec_score[i+1]=spec\n",
    "    \n",
    "  return spec_score\n",
    "    \n",
    "data=Specificity_Score(cv_model,y_val)\n",
    "stat.mean(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPR_Score(cv_model,y_test):\n",
    "  false_positive_rate = {}\n",
    "  \n",
    "  for i, model in enumerate(cv_model):\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_val.values, prediction_type='Class')\n",
    "    # Calculate accuracy\n",
    "    fpr = 1-recall_score(y_val, y_pred, pos_label=0)\n",
    "    false_positive_rate[i+1]=fpr\n",
    "    \n",
    "  return false_positive_rate\n",
    "    \n",
    "data=FPR_Score(cv_model,y_val)\n",
    "stat.mean(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def ROC_AUC_score(cv_model,y_test):\n",
    "  RA_score ={}\n",
    "  for i, model in enumerate(cv_model):\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_val.values, prediction_type='Class')\n",
    "    # Calculate accuracy\n",
    "    accuracy = roc_auc_score(y_val, y_pred)\n",
    "    RA_score[i+1]=accuracy\n",
    "    \n",
    "  return RA_score\n",
    "    \n",
    "data=ROC_AUC_score(cv_model,y_test)\n",
    "stat.mean(data.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
